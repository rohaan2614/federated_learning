{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nimport tarfile\nfrom torchvision.datasets.utils import download_url\nfrom torch.utils.data import random_split, DataLoader\n\nfrom torchvision.datasets import CIFAR10\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = [5, 5]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-09T17:23:44.299453Z","iopub.execute_input":"2024-08-09T17:23:44.300095Z","iopub.status.idle":"2024-08-09T17:23:44.308488Z","shell.execute_reply.started":"2024-08-09T17:23:44.300059Z","shell.execute_reply":"2024-08-09T17:23:44.307400Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n])\n\ntrain_dataset = CIFAR10('data', train=True, download=True, transform=transform)\ntest_dataset = CIFAR10('data', train=False, download=True, transform=transform)\n\ntrain_dataset, dev_dataset = random_split(train_dataset, [int(len(train_dataset) * 0.80), int(len(train_dataset) * 0.2)])","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:23:44.310665Z","iopub.execute_input":"2024-08-09T17:23:44.311004Z","iopub.status.idle":"2024-08-09T17:23:45.990815Z","shell.execute_reply.started":"2024-08-09T17:23:44.310977Z","shell.execute_reply":"2024-08-09T17:23:45.989870Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"code","source":"total_train_size = len(train_dataset)\ntotal_test_size = len(test_dataset)\ntotal_dev_size = len(dev_dataset)\ntotal_train_size, total_dev_size, total_test_size","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:23:45.992066Z","iopub.execute_input":"2024-08-09T17:23:45.992433Z","iopub.status.idle":"2024-08-09T17:23:45.999866Z","shell.execute_reply.started":"2024-08-09T17:23:45.992406Z","shell.execute_reply":"2024-08-09T17:23:45.998623Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"(40000, 10000, 10000)"},"metadata":{}}]},{"cell_type":"code","source":"classes = 10\ninput_dim = 3 * 32 * 32\n\nnum_clients = 8\nrounds = 15\nbatch_size = 64\nepochs_per_client = 5\nlearning_rate = 0.05","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:23:46.002438Z","iopub.execute_input":"2024-08-09T17:23:46.002835Z","iopub.status.idle":"2024-08-09T17:23:46.008456Z","shell.execute_reply.started":"2024-08-09T17:23:46.002807Z","shell.execute_reply":"2024-08-09T17:23:46.007213Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def get_device():\n    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndef to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader(DataLoader):\n        def __init__(self, dl, device):\n            self.dl = dl\n            self.device = device\n\n        def __iter__(self):\n            for batch in self.dl:\n                yield to_device(batch, self.device)\n\n        def __len__(self):\n            return len(self.dl)\n\ndevice = get_device()\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:23:46.009632Z","iopub.execute_input":"2024-08-09T17:23:46.009986Z","iopub.status.idle":"2024-08-09T17:23:46.022179Z","shell.execute_reply.started":"2024-08-09T17:23:46.009947Z","shell.execute_reply":"2024-08-09T17:23:46.021162Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport copy\n\nclass FederatedNet(torch.nn.Module):\n    def __init__(self):\n        super(FederatedNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 10)\n        self.dropout = nn.Dropout(0.5)\n        self.track_layers = {\n        'conv1': self.conv1,\n        'conv2': self.conv2,\n        'conv3': self.conv3,\n        'fc1': self.fc1,\n        'fc2': self.fc2,\n        'fc3': self.fc3\n    }\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, 128 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n\n    def get_track_layers(self):\n        return self.track_layers\n\n    def apply_parameters(self, parameters_dict):\n        with torch.no_grad():\n            for layer_name in parameters_dict:\n                self.track_layers[layer_name].weight.data *= 0\n                self.track_layers[layer_name].bias.data *= 0\n                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n\n    def get_parameters(self, deep_copy = True):\n        parameters_dict = dict()\n        for layer_name in self.track_layers:\n            parameters_dict[layer_name] = {\n                'weight': self.track_layers[layer_name].weight.data,\n                'bias': self.track_layers[layer_name].bias.data\n            }\n        if deep_copy:\n            params_clone = copy.deepcopy(parameters_dict)\n            return params_clone\n        else:\n            return parameters_dict\n\n    def batch_accuracy(self, outputs, labels):\n        with torch.no_grad():\n            _, predictions = torch.max(outputs, dim=1)\n            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n\n    def _process_batch(self, batch):\n        images, labels = batch\n        outputs = self(images)\n        loss = torch.nn.functional.cross_entropy(outputs, labels)\n        accuracy = self.batch_accuracy(outputs, labels)\n        return (loss, accuracy)\n\n    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n        optimizer = opt(self.parameters(), lr)\n        history = []\n        for epoch in range(epochs):\n            losses = []\n            accs = []\n            for batch in dataloader:\n                loss, acc = self._process_batch(batch)\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n                loss.detach()\n                losses.append(loss)\n                accs.append(acc)\n            avg_loss = torch.stack(losses).mean().item()\n            avg_acc = torch.stack(accs).mean().item()\n            history.append((avg_loss, avg_acc))\n        return history\n\n    def evaluate(self, dataset, batch_size=128):\n        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n        losses = []\n        accs = []\n        with torch.no_grad():\n            for batch in dataloader:\n                loss, acc = self._process_batch(batch)\n                losses.append(loss)\n                accs.append(acc)\n        avg_loss = torch.stack(losses).mean().item()\n        avg_acc = torch.stack(accs).mean().item()\n        return (avg_loss, avg_acc)","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:23:46.023713Z","iopub.execute_input":"2024-08-09T17:23:46.024333Z","iopub.status.idle":"2024-08-09T17:23:46.049886Z","shell.execute_reply.started":"2024-08-09T17:23:46.024306Z","shell.execute_reply":"2024-08-09T17:23:46.049013Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class Client:\n    def __init__(self, client_id, dataset):\n        self.client_id = client_id\n        self.dataset = dataset\n\n    def get_dataset_size(self):\n        return len(self.dataset)\n\n    def get_client_id(self):\n        return self.client_id\n\n    def __difference(self, params1 : dict, params2 : dict):\n        diff = {}\n        for layer in params1.keys():\n            diff[layer] = {}\n            for key in params1[layer].keys():\n                diff[layer][key] = params1[layer][key] - params2[layer][key]\n        return diff\n\n    def train(self, parameters_dict):\n        net = to_device(FederatedNet(), device)\n        net.apply_parameters(parameters_dict)\n        wt = net.get_parameters(deep_copy=True)\n        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n        loss = round(train_history[-1][0],3)\n        accuracy = round(train_history[-1][1],3)\n        print(f'{self.client_id}, Loss = {loss}, Accuracy = {accuracy}')\n        wt_plus_1 = net.get_parameters(deep_copy=True)\n        update = self.__difference(wt, wt_plus_1)\n        return update","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:23:46.051102Z","iopub.execute_input":"2024-08-09T17:23:46.051408Z","iopub.status.idle":"2024-08-09T17:23:46.061340Z","shell.execute_reply.started":"2024-08-09T17:23:46.051383Z","shell.execute_reply":"2024-08-09T17:23:46.060471Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"examples_per_client = total_train_size // num_clients\nclient_datasets = random_split(train_dataset, [min(i + examples_per_client,\n           total_train_size) - i for i in range(0, total_train_size, examples_per_client)])\nclients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:23:46.062394Z","iopub.execute_input":"2024-08-09T17:23:46.062678Z","iopub.status.idle":"2024-08-09T17:23:46.075701Z","shell.execute_reply.started":"2024-08-09T17:23:46.062654Z","shell.execute_reply":"2024-08-09T17:23:46.075013Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def diff(wt, wt_plus_1):\n    for layer in wt.keys():\n        for key in wt[layer].keys():\n            print(wt[layer][key] - wt_plus_1[layer][key])","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:23:46.077126Z","iopub.execute_input":"2024-08-09T17:23:46.077516Z","iopub.status.idle":"2024-08-09T17:23:46.082670Z","shell.execute_reply.started":"2024-08-09T17:23:46.077486Z","shell.execute_reply":"2024-08-09T17:23:46.081656Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def reconstruct_wt_plus_1(global_params, client_updates):\n    wt_plus_1 = {}\n    for layer in global_params.keys():\n        wt_plus_1[layer] = {}\n        for key in global_params[layer].keys():\n            wt_plus_1[layer][key] = global_params[layer][key] - client_updates[layer][key]\n    return wt_plus_1","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:23:46.086155Z","iopub.execute_input":"2024-08-09T17:23:46.086484Z","iopub.status.idle":"2024-08-09T17:23:46.091987Z","shell.execute_reply.started":"2024-08-09T17:23:46.086461Z","shell.execute_reply":"2024-08-09T17:23:46.091019Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef flatten_dict_to_vector(d):\n    flat_list = []\n    shapes = {}\n\n    for k1, v1 in d.items():\n        for k2, tensor in v1.items():\n            shapes[f'{k1}_{k2}'] = tensor.shape\n            # Check if tensor is on GPU and move to CPU if necessary\n            if tensor.is_cuda:\n                tensor = tensor.cpu()\n            flat_list.extend(tensor.flatten().numpy())\n\n    flat_vector = np.array(flat_list)\n    return flat_vector, shapes","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:23:46.093077Z","iopub.execute_input":"2024-08-09T17:23:46.093337Z","iopub.status.idle":"2024-08-09T17:23:46.101166Z","shell.execute_reply.started":"2024-08-09T17:23:46.093315Z","shell.execute_reply":"2024-08-09T17:23:46.100050Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def restore_vector_to_dict(flat_vector, shapes):\n    restored_dict = {}\n    offset = 0\n\n    # Check if CUDA is available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    for k, shape in shapes.items():\n        size = np.prod(shape)\n        tensor_flat = flat_vector[offset:offset + size]\n        tensor = tensor_flat.clone().detach().reshape(shape).to(device)  # Send tensor to the appropriate device\n\n        k1, k2 = k.split('_')\n        if k1 not in restored_dict:\n            restored_dict[k1] = {}\n        restored_dict[k1][k2] = tensor\n\n        offset += size\n\n    return restored_dict","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:31:19.072674Z","iopub.execute_input":"2024-08-09T17:31:19.073523Z","iopub.status.idle":"2024-08-09T17:31:19.080120Z","shell.execute_reply.started":"2024-08-09T17:31:19.073489Z","shell.execute_reply":"2024-08-09T17:31:19.079188Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from time import time\nstart = time()\nglobal_net = to_device(FederatedNet(), device)\nhistory = []\nfor i in range(rounds):\n    print('Start Round {} ...'.format(i + 1))\n    curr_parameters = global_net.get_parameters()\n    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n    updates = []\n\n     # get client updates\n    for client in clients:\n        update = client.train(curr_parameters)\n        updates.append(update)\n    processed_updates = []\n\n    # apply ga to updates\n    for update in updates:\n        flat_vector, shapes = flatten_dict_to_vector(update)\n        d = flat_vector.size\n        q = 1000\n        random_seed = 42\n\n        np.random.seed(random_seed)\n        G_np = np.random.normal(0, 1, size=(d, int(q)))\n        G = torch.tensor(G_np, dtype=torch.float32)\n        w = torch.matmul(G.T, torch.tensor(flat_vector)) / q\n        delta = torch.matmul(G, w)\n        restored_wt = restore_vector_to_dict(delta, shapes)\n        processed_updates.append(restored_wt)\n\n    # send updates to the server\n    for update in processed_updates:\n        client_parameters = reconstruct_wt_plus_1(curr_parameters, update)\n        fraction = client.get_dataset_size() / total_train_size\n        for layer_name in client_parameters:\n            new_parameters[layer_name]['weight'] += fraction * (client_parameters[layer_name]['weight'])\n            new_parameters[layer_name]['bias'] += fraction * (client_parameters[layer_name]['bias'])\n    global_net.apply_parameters(new_parameters)\n\n    train_loss, train_acc = global_net.evaluate(train_dataset)\n    dev_loss, dev_acc = global_net.evaluate(dev_dataset)\n    print('After round {}, train_loss = {}, dev_loss = {}, dev_acc = {}\\n'.format(i + 1, round(train_loss, 4),\n            round(dev_loss, 4), round(dev_acc, 4)))\n    history.append((train_loss, dev_loss, dev_acc))","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:31:23.097453Z","iopub.execute_input":"2024-08-09T17:31:23.097828Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Start Round 1 ...\nclient_0, Loss = 1.935, Accuracy = 0.296\nclient_1, Loss = 1.914, Accuracy = 0.289\nclient_2, Loss = 1.931, Accuracy = 0.275\nclient_3, Loss = 1.927, Accuracy = 0.278\nclient_4, Loss = 1.916, Accuracy = 0.295\nclient_5, Loss = 1.952, Accuracy = 0.278\nclient_6, Loss = 1.926, Accuracy = 0.29\nclient_7, Loss = 1.928, Accuracy = 0.291\nAfter round 1, train_loss = 13.5183, dev_loss = 13.5761, dev_acc = 0.1128\n\nStart Round 2 ...\nclient_0, Loss = 1.826, Accuracy = 0.321\nclient_1, Loss = 1.741, Accuracy = 0.35\nclient_2, Loss = 1.741, Accuracy = 0.34\nclient_3, Loss = 1.735, Accuracy = 0.349\nclient_4, Loss = 1.725, Accuracy = 0.365\nclient_5, Loss = 1.728, Accuracy = 0.359\nclient_6, Loss = 1.746, Accuracy = 0.357\nclient_7, Loss = 1.735, Accuracy = 0.35\nAfter round 2, train_loss = 586.3163, dev_loss = 588.2312, dev_acc = 0.0942\n\nStart Round 3 ...\nclient_0, Loss = nan, Accuracy = 0.102\nclient_1, Loss = nan, Accuracy = 0.1\nclient_2, Loss = nan, Accuracy = 0.111\nclient_3, Loss = nan, Accuracy = 0.092\nclient_4, Loss = nan, Accuracy = 0.096\nclient_5, Loss = nan, Accuracy = 0.1\nclient_6, Loss = nan, Accuracy = 0.103\nclient_7, Loss = nan, Accuracy = 0.099\nAfter round 3, train_loss = nan, dev_loss = nan, dev_acc = 0.1023\n\nStart Round 4 ...\nclient_0, Loss = nan, Accuracy = 0.101\nclient_1, Loss = nan, Accuracy = 0.098\nclient_2, Loss = nan, Accuracy = 0.105\nclient_3, Loss = nan, Accuracy = 0.092\nclient_4, Loss = nan, Accuracy = 0.096\nclient_5, Loss = nan, Accuracy = 0.101\nclient_6, Loss = nan, Accuracy = 0.097\nclient_7, Loss = nan, Accuracy = 0.099\nAfter round 4, train_loss = nan, dev_loss = nan, dev_acc = 0.1023\n\nStart Round 5 ...\nclient_0, Loss = nan, Accuracy = 0.102\nclient_1, Loss = nan, Accuracy = 0.096\nclient_2, Loss = nan, Accuracy = 0.108\nclient_3, Loss = nan, Accuracy = 0.093\nclient_4, Loss = nan, Accuracy = 0.099\nclient_5, Loss = nan, Accuracy = 0.101\nclient_6, Loss = nan, Accuracy = 0.098\nclient_7, Loss = nan, Accuracy = 0.101\nAfter round 5, train_loss = nan, dev_loss = nan, dev_acc = 0.1023\n\nStart Round 6 ...\nclient_0, Loss = nan, Accuracy = 0.101\nclient_1, Loss = nan, Accuracy = 0.096\nclient_2, Loss = nan, Accuracy = 0.105\nclient_3, Loss = nan, Accuracy = 0.093\nclient_4, Loss = nan, Accuracy = 0.096\nclient_5, Loss = nan, Accuracy = 0.101\nclient_6, Loss = nan, Accuracy = 0.1\nclient_7, Loss = nan, Accuracy = 0.099\nAfter round 6, train_loss = nan, dev_loss = nan, dev_acc = 0.1023\n\nStart Round 7 ...\nclient_0, Loss = nan, Accuracy = 0.102\nclient_1, Loss = nan, Accuracy = 0.1\nclient_2, Loss = nan, Accuracy = 0.105\nclient_3, Loss = nan, Accuracy = 0.093\nclient_4, Loss = nan, Accuracy = 0.097\nclient_5, Loss = nan, Accuracy = 0.1\nclient_6, Loss = nan, Accuracy = 0.097\nclient_7, Loss = nan, Accuracy = 0.099\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Execution lasted:\", time.time()-start)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([i + 1 for i in range(len(history))], [history[i][0] for i in range(len(history))], color='r', label='train loss')\nplt.plot([i + 1 for i in range(len(history))], [history[i][1] for i in range(len(history))], color='b', label='dev loss')\nplt.legend()\nplt.title('Training history')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:25:23.461153Z","iopub.status.idle":"2024-08-09T17:25:23.461512Z","shell.execute_reply.started":"2024-08-09T17:25:23.461335Z","shell.execute_reply":"2024-08-09T17:25:23.461350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dev_accs = [history[i][2] for i in range(len(history))]\n# Plot accuracies\nplt.plot([i + 1 for i in range(len(history))], dev_accs, color='m', label='dev accuracy')\nplt.legend()\nplt.title('Accuracy history')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-09T17:25:23.463035Z","iopub.status.idle":"2024-08-09T17:25:23.463416Z","shell.execute_reply.started":"2024-08-09T17:25:23.463228Z","shell.execute_reply":"2024-08-09T17:25:23.463245Z"},"trusted":true},"execution_count":null,"outputs":[]}]}